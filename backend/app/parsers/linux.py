"""
Linux SAR file parser.

This module provides the parser for Linux SAR file format.
"""

import re
from datetime import datetime, time
from typing import List, Dict, Any, Optional
from .base import BaseParser, TimeSeriesData, ParsedData
import logging

logger = logging.getLogger(__name__)


class LinuxParser(BaseParser):
    """
    Parser for Linux SAR files.
    
    This parser handles the Linux SAR file format as generated by the sysstat package.
    """

    def __init__(self):
        """Initialize the Linux parser."""
        super().__init__()
        self.parser_name = "Linux"
        self.system_info.os_type = "Linux"
        
        # Lines to ignore based on Java implementation
        self.ignore_lines_beginning_with = {
            "##",
            "Average:", "Summary:", "Summary",
            "Moyenne :", "Moyenne?:", "Résumé:",
            "Durchschn.:", "Zusammenfassung:",
            "Media:", "Resumen:"
        }
        
        # Initialize metrics storage
        self.metrics: Dict[str, TimeSeriesData] = {}
        
        # Graph configurations for different metrics
        self.graph_configs = {
            "cpu": {
                "title": "CPU Utilization",
                "type": "line",
                "metrics": ["%user", "%nice", "%system", "%iowait", "%steal", "%idle"]
            },
            "memory": {
                "title": "Memory Usage",
                "type": "line", 
                "metrics": ["kbmemfree", "kbmemused", "%memused", "kbbuffers", "kbcached"]
            },
            "disk": {
                "title": "Disk I/O",
                "type": "line",
                "metrics": ["tps", "rtps", "wtps", "bread/s", "bwrtn/s"]
            },
            "network": {
                "title": "Network I/O",
                "type": "line",
                "metrics": ["rxpck/s", "txpck/s", "rxkB/s", "txkB/s"]
            }
        }

    def parse_header(self, header_line: str) -> None:
        """
        Parse the Linux SAR header line.
        
        Expected format: Linux 3.10.0-1160.53.1.el7.x86_64 (hostname) 01/03/23 _x86_64_ (8 CPU)
        
        Args:
            header_line: The header line to parse
        """
        logger.debug(f"Header Line: {header_line}")
        
        # Split the header line into columns
        columns = header_line.split()
        
        if len(columns) < 4:
            logger.error(f"Invalid header line format: {header_line}")
            return
        
        # Extract OS type and kernel
        self.system_info.os_type = columns[0]
        self.system_info.kernel = columns[1]
        
        # Extract hostname (remove surrounding brackets)
        hostname_match = re.search(r'\((.*?)\)', header_line)
        if hostname_match:
            self.system_info.hostname = hostname_match.group(1)
        
        # Extract date
        date_str = columns[3] if len(columns) > 3 else None
        if date_str:
            self.check_date_format()
            self.set_date(date_str)
        
        # Extract CPU count if available
        cpu_match = re.search(r'\((\d+)\s+CPU\)', header_line)
        if cpu_match:
            self.system_info.nb_cpu = cpu_match.group(1)

    def check_date_format(self) -> None:
        """Check and set the date format for Linux SAR files."""
        # For Linux, we'll use automatic detection by default
        self.date_format = "Automatic Detection"
        self.time_column = 0

    def parse_line(self, line: str, columns: List[str]) -> int:
        """
        Parse a single line of Linux SAR data.
        
        Args:
            line: The raw line to parse
            columns: The line split into columns
            
        Returns:
            int: Status code (0=success, -1=error, 1=ignore, 2=graph data, 3=no graph)
        """
        if not columns:
            return -1
        
        # Check if line should be ignored
        if self.should_ignore_line(columns):
            self.current_stat = "NONE"
            return 1
        
        # Check for LINUX RESTART
        if "LINUX RESTART" in line:
            return 1
        
        # Parse time
        try:
            if self.time_column == 0:
                # Try to detect time format
                if len(columns) > 1 and re.match(r'^\d{2}:\d{2}:\d{2}\s[AP]M$', f"{columns[0]} {columns[1]}"):
                    self.time_format = "%I:%M:%S %p"
                    self.time_column = 2
                else:
                    self.time_column = 1
            
            # Parse the time
            if self.time_column == 2 and len(columns) > 1:
                time_str = f"{columns[0]} {columns[1]}"
                self.parse_time = datetime.strptime(time_str, self.time_format).time()
            else:
                self.parse_time = datetime.strptime(columns[0], self.time_format).time()
            
            # Create datetime object
            if self.parse_date and self.parse_time:
                now_stat = datetime.combine(self.parse_date, self.parse_time)
            else:
                raise ValueError("date/time is missing")
            
            self.set_start_and_end_of_graph(now_stat)
            self.first_data_column = self.time_column
            
        except (ValueError, IndexError) as ex:
            logger.error(f"Unable to parse time {columns[0] if columns else 'N/A'}: {ex}")
            return -1
        
        # Determine the statistic type based on columns
        stat_type = self.determine_stat_type(columns)
        if stat_type:
            logger.debug(f"Setting current_stat to: {stat_type}")
            self.current_stat = stat_type
            return 2
        
        # If we have a current stat, try to parse the data
        if self.current_stat != "NONE" and self.current_stat != "IGNORE":
            return self.parse_stat_data(now_stat, line, columns)
        
        return -1

    def determine_stat_type(self, columns: List[str]) -> Optional[str]:
        """
        Determine the type of statistic based on the column headers.
        
        Args:
            columns: The line split into columns
            
        Returns:
            Optional[str]: The statistic type or None if not recognized
        """
        if len(columns) < 2:
            return None
        
        # Check for CPU statistics
        if columns[0] == "CPU" and any("%user" in col or "%usr" in col for col in columns):
            logger.debug(f"Detected CPU stat type: {columns}")
            return "cpu"
        
        # Check for memory statistics
        if any(col in " ".join(columns) for col in ["kbmemfree", "kbmemused", "%memused"]):
            return "memory"
        
        # Check for disk statistics
        if any(col in " ".join(columns) for col in ["DEV", "tps", "rd_sec/s", "wr_sec/s"]):
            return "disk"
        
        # Check for network statistics
        if any(col in " ".join(columns) for col in ["IFACE", "rxpck/s", "txpck/s"]):
            return "network"
        
        # Check for load statistics
        if any(col in " ".join(columns) for col in ["runq-sz", "plist-sz", "ldavg-1"]):
            return "load"
        
        return None

    def parse_stat_data(self, timestamp: datetime, line: str, columns: List[str]) -> int:
        """
        Parse the actual data for a specific statistic type.
        
        Args:
            timestamp: The timestamp for this data point
            line: The raw line
            columns: The line split into columns
            
        Returns:
            int: Status code
        """
        if self.current_stat == "IGNORE":
            return 0
        
        if self.current_stat == "NONE":
            return -1
        
        # Add timestamp to date samples
        self.date_samples.add(timestamp)
        
        # Parse the data based on the current stat type
        try:
            if self.current_stat == "cpu":
                return self.parse_cpu_data(timestamp, columns)
            elif self.current_stat == "memory":
                return self.parse_memory_data(timestamp, columns)
            elif self.current_stat == "disk":
                return self.parse_disk_data(timestamp, columns)
            elif self.current_stat == "network":
                return self.parse_network_data(timestamp, columns)
            elif self.current_stat == "load":
                return self.parse_load_data(timestamp, columns)
            else:
                return 3  # No graph associated
                
        except (ValueError, IndexError) as ex:
            logger.error(f"Error parsing {self.current_stat} data: {ex}")
            return -1

    def parse_cpu_data(self, timestamp: datetime, columns: List[str]) -> int:
        """Parse CPU utilization data."""
        logger.debug(f"parse_cpu_data called with columns: {columns}")
        if len(columns) < self.first_data_column + 1:
            logger.debug(f"Not enough columns: {len(columns)} < {self.first_data_column + 1}")
            return -1
        
        # Skip header lines
        if columns[self.first_data_column] == "CPU":
            logger.debug("Skipping CPU header line")
            return 0
        
        # Parse CPU data lines (like "all" or specific CPU numbers)
        if columns[self.first_data_column] in ["all", "0", "1", "2", "3", "4", "5", "6", "7"]:
            logger.debug(f"Processing CPU data line for: {columns[self.first_data_column]}")
        else:
            logger.debug(f"Unknown CPU identifier: {columns[self.first_data_column]}")
            return 0
        
        # Parse CPU values
        try:
            cpu_values = []
            for i in range(self.first_data_column + 1, len(columns)):
                try:
                    value = float(columns[i])
                    cpu_values.append(value)
                except ValueError:
                    break
            
            # Store the data
            if cpu_values:
                metric_name = f"cpu_{columns[self.first_data_column]}"
                if metric_name not in self.metrics:
                    self.metrics[metric_name] = TimeSeriesData(
                        metric_name=metric_name,
                        columns=["%usr", "%nice", "%sys", "%iowait", "%steal", "%irq", "%soft", "%guest", "%gnice", "%idle"][:len(cpu_values)]
                    )
                
                self.metrics[metric_name].timestamps.append(timestamp)
                self.metrics[metric_name].values.extend(cpu_values)
            
            return 2
            
        except (ValueError, IndexError) as ex:
            logger.error(f"Error parsing CPU data: {ex}")
            return -1

    def parse_memory_data(self, timestamp: datetime, columns: List[str]) -> int:
        """Parse memory usage data."""
        if len(columns) < self.first_data_column + 1:
            return -1
        
        try:
            # Parse memory values (simplified - you may need to adjust based on actual format)
            memory_values = []
            for i in range(self.first_data_column + 1, len(columns)):
                try:
                    value = float(columns[i])
                    memory_values.append(value)
                except ValueError:
                    break
            
            if memory_values:
                metric_name = "memory"
                if metric_name not in self.metrics:
                    self.metrics[metric_name] = TimeSeriesData(
                        metric_name=metric_name,
                        columns=["kbmemfree", "kbmemused", "%memused", "kbbuffers", "kbcached"][:len(memory_values)]
                    )
                
                self.metrics[metric_name].timestamps.append(timestamp)
                self.metrics[metric_name].values.extend(memory_values)
            
            return 2
            
        except (ValueError, IndexError) as ex:
            logger.error(f"Error parsing memory data: {ex}")
            return -1

    def parse_disk_data(self, timestamp: datetime, columns: List[str]) -> int:
        """Parse disk I/O data."""
        if len(columns) < self.first_data_column + 1:
            return -1
        
        try:
            # Parse disk values
            disk_values = []
            for i in range(self.first_data_column + 1, len(columns)):
                try:
                    value = float(columns[i])
                    disk_values.append(value)
                except ValueError:
                    break
            
            if disk_values:
                metric_name = f"disk_{columns[self.first_data_column]}"
                if metric_name not in self.metrics:
                    self.metrics[metric_name] = TimeSeriesData(
                        metric_name=metric_name,
                        columns=["tps", "rtps", "wtps", "bread/s", "bwrtn/s"][:len(disk_values)]
                    )
                
                self.metrics[metric_name].timestamps.append(timestamp)
                self.metrics[metric_name].values.extend(disk_values)
            
            return 2
            
        except (ValueError, IndexError) as ex:
            logger.error(f"Error parsing disk data: {ex}")
            return -1

    def parse_network_data(self, timestamp: datetime, columns: List[str]) -> int:
        """Parse network I/O data."""
        if len(columns) < self.first_data_column + 1:
            return -1
        
        try:
            # Parse network values
            network_values = []
            for i in range(self.first_data_column + 1, len(columns)):
                try:
                    value = float(columns[i])
                    network_values.append(value)
                except ValueError:
                    break
            
            if network_values:
                metric_name = f"network_{columns[self.first_data_column]}"
                if metric_name not in self.metrics:
                    self.metrics[metric_name] = TimeSeriesData(
                        metric_name=metric_name,
                        columns=["rxpck/s", "txpck/s", "rxkB/s", "txkB/s"][:len(network_values)]
                    )
                
                self.metrics[metric_name].timestamps.append(timestamp)
                self.metrics[metric_name].values.extend(network_values)
            
            return 2
            
        except (ValueError, IndexError) as ex:
            logger.error(f"Error parsing network data: {ex}")
            return -1

    def parse_load_data(self, timestamp: datetime, columns: List[str]) -> int:
        """Parse load average data."""
        if len(columns) < self.first_data_column + 1:
            return -1
        
        try:
            # Parse load values
            load_values = []
            for i in range(self.first_data_column + 1, len(columns)):
                try:
                    value = float(columns[i])
                    load_values.append(value)
                except ValueError:
                    break
            
            if load_values:
                metric_name = "load"
                if metric_name not in self.metrics:
                    self.metrics[metric_name] = TimeSeriesData(
                        metric_name=metric_name,
                        columns=["runq-sz", "plist-sz", "ldavg-1", "ldavg-5", "ldavg-15"][:len(load_values)]
                    )
                
                self.metrics[metric_name].timestamps.append(timestamp)
                self.metrics[metric_name].values.extend(load_values)
            
            return 2
            
        except (ValueError, IndexError) as ex:
            logger.error(f"Error parsing load data: {ex}")
            return -1

    def get_parsed_data(self, file_id: str) -> ParsedData:
        """
        Get the parsed data as a ParsedData object.
        
        Args:
            file_id: The ID of the file being parsed
            
        Returns:
            ParsedData: The parsed data
        """
        return ParsedData(
            file_id=file_id,
            system_info=self.system_info,
            start_time=self.start_of_graph,
            end_time=self.end_of_graph,
            metrics=self.metrics,
            date_samples=self.date_samples
        ) 